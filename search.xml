<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>线性模型2</title>
    <url>/2020/02/12/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B2/</url>
    <content><![CDATA[<h2 id="类别"><a href="#类别" class="headerlink" title="类别"></a>类别</h2><h3 id="线性模型中的多分类学习"><a href="#线性模型中的多分类学习" class="headerlink" title="线性模型中的多分类学习"></a>线性模型中的多分类学习</h3><p>现实中常遇到多分类学习任务.有些二分类学习方法可直接推广到多分类，但在更多情形下，我们是基于一些基本策略，利用二分类学习器来解决多分类问题。</p>
<p> 多分类学习的基本思路是”拆解法”，即将多分类任务拆为若干个二分类任务求解.具体来说，先对问题进行拆分，然后为拆出的每个二分类任务训练一个分类器;在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果.这里的关键是如何对多分类任务进行拆分，以及如何对多个分类器进行集成.最经典的拆分策略有三种. “一对一” (One vs. One，简称OvO) 、”一对其余” (One vs. Rest ，简称OvR)和”多对多” (Many vs. Many，简称MvM)。</p>
<h4 id="OvO与OvR"><a href="#OvO与OvR" class="headerlink" title="OvO与OvR"></a>OvO与OvR</h4><p>OvO 将N 个类别两两配对，从而产生N(N 一1)/2 个二分类任务，最终结果可通过投票产生。OvR 则是每次将一个类的样例作为正例、所有其他类的样例作为反例来训练N个分类器.在测试时若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果。若有多个分类器预测为正类，则通常考虑各分类器的预测置信度，选择置信度最大的类别标记作为分类结果。下图为OvO与OvR的示意图：</p>
<img src="/2020/02/12/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B2/OvO与OvR.png" style="zoom:80%;">



<p>OvR 只需训练N个分类器， 而OvO 需训练N(N - 1)/2 个分类器， 因此， OvO的存储开销和测试间开销通常比OvR 更大. 但在训练时，OvR 的每个分类器均使用全部训练样例，而OvO 的每个分类器仅用到两个类的样例，因此，在类别很多时，OvO 的训练时间开销通常比OvR 更小. 预测性能在多数情形下两者差不多.</p>
<h4 id="MvM"><a href="#MvM" class="headerlink" title="MvM"></a>MvM</h4><p>MvM 是每次将若干个类作为正类，若干个其他类作为反类. MvM 的正、反类构造必须有特殊的设计，不能随意选取.最常用的MvM 技术是”纠错输出码” (Error CorrectingOutput Codes，简称ECOC).它尽可能在解码过程中具有容错性. </p>
<h5 id="Error-CorrectingOutput-Codes-ECOC"><a href="#Error-CorrectingOutput-Codes-ECOC" class="headerlink" title="Error CorrectingOutput Codes(ECOC)"></a>Error CorrectingOutput Codes(ECOC)</h5><p>ECOC 工作过程主要分为两步:<br>编码:对N个类别做M次划分， 每次划分将一部分类别划为正类，一部分划为反类，从而形成一个二分类训练集;这样一共产生M个训练集，可训练出M个分类器.<br>解码:M个分类器分别对测试样本进行预测，这些预测标记组成一个编码.将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果.</p>
<img src="/2020/02/12/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B2/ECOC.png" style="zoom:80%;">

<p>ECOC 编码示意图”+1” 、”-1” 分别表示学习器f将该类样本作为正、反例;三元码中”0” 表示f不使用该类样本<br>一般来说，对同一个学习任务， ECOC 编码越长，纠错能力越强.然而，编码越长，意味着所需训练的分类器越多，计算、存储开销都会增大;另外，对有限类别数，可能的组合数目是有限的。</p>
<p>对OvR 、MvM 来说，由于对每个类进行了相同的处理，其拆解出的二分类任务中类别不平衡的影响会相互抵消，因此通常不需专门处理.</p>
<h3 id="类别不平衡问题-class-imbalance"><a href="#类别不平衡问题-class-imbalance" class="headerlink" title="类别不平衡问题(class-imbalance)"></a>类别不平衡问题(class-imbalance)</h3><p>如果不同类别的训练样例数目稍有差别，通常影响不大，但若差别很大，则会对学习过程造成困扰.例如有998个反例，但正例只有2个，那么学习方法只需返回一个永远将新样本预测为反例的学习器，就能达到99.8%的精度;然而这样的学习器往往没有价值，因为它不能预测出任何正例.</p>
<h4 id="再缩放"><a href="#再缩放" class="headerlink" title="再缩放"></a>再缩放</h4><p>再缩放，又称为再平衡，是不平衡学习的一个基本策略。</p>
<p>通常在二分类情况下，我们实际上是在使用预测出的y值与阈值比较从而获得分类的结果：当y&gt;0.5，我们称之为正例，y&lt;=0.5，我们称之为反例。即：y/(1-y)&gt;1为正例。<br>再缩放的基本思想，即正例： y/(1-y)&gt;(m+)/(m-) ；m+为正样本数量 ，m-为负样本数量。因此运用到实际预测当中时，有：f= y/(1-y) * (m+)/(m-) </p>
<p>但是，我们未必能有效地基于训练集观测几率来准确地推断出真实几率.因此现有技术大体上有三类做法:第一类是直接对训练集里的反类样例进行”欠采样” (undersampling) ，即去除一些反倒使得正、反例数日接近然后再进行学习;<br>第二类是对训练集里的正类样例进行”过来样” (oversampling) ，即增加一些正例使得正、反例数目接近，然后再进行学习;<br>第三类则是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将式(3.48)嵌入到其决策过程中，称为”阔值移动” (threshold-moving).</p>
<p>欠采样法的时间开销通常远小于过采样法，因为前者丢弃了很多反例，使得分类器训练集远小子初始训练集，而过来样法增加了很多正例，其训练集大于初始训练集.<br>需注意的是，过采样法不能简单地对初始正例样本进行重复来样，否则会招致严重的过拟合，过采样法的代表性算法SMOTE是通过对训练集里的正例进行插值来产生额外的正例.<br>另一方面，欠采样法若随机丢弃反例可能丢失一些重要信息;欠采样法的代表性算法EasyEnsemble则是利用集成学习机制，将反例划分为若干个集合供不同学习器使用，这样对每个学习器来看都进行了欠采样，但在全局来看却不会丢失重要信息.</p>
<p>最后值得一提的是，”再缩放”也是”代价敏感学习” (cost-sensitive learning) 的基础。代价敏感学习为不同类的错误设定了不同的权值，其错误率计算为：<br><img src="/2020/02/12/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B2/代价敏感学习错误率.png" style="zoom:80%;"><br>cost即为被预测为不同类预测错误时所需付出的代价。</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>线性模型1</title>
    <url>/2020/02/10/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B1/</url>
    <content><![CDATA[<h2 id="线性模型的基本形式"><a href="#线性模型的基本形式" class="headerlink" title="线性模型的基本形式"></a>线性模型的基本形式</h2><img src="/2020/02/10/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B1/基本形式.png" style="zoom:100%;">

<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>“线性回归” (linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记。通常使用均方误差，亦称平方损失，来作为性能度量，试图使均方误差最小。</p>
<p>基于均方误差最小化来进行模型求解的方法称为”最小二乘法” (least suare method). 在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小.(最小二乘法用途很广，不仅限于线性回归)</p>
<p>在求解使得均方差最小化的过程中，称为线性回归模型对最小二乘的参数估计。我们可对其均方差求导，另求导式为零，则可得w与b的最优解。</p>
<p>然而现实中，往往会求出多个解。这时选择哪一个作为解进行输出，则由算法的归纳偏好决定。常见的做法是引入正则化(regularization) 项.如，假设我们认为示例所对应的输出标记是在指数尺度上变化，则可以使得 lny = wx +b 。这就是”对数线性回归” (log-linear regression)。</p>
<p>更一般地，考虑单调可微函数g(.) ， 令 g(y) = wx + b , 这样得到的模型称为” 广义线性模型” (generalized linear model) ，其中函数g() 称为”联系函数” (link function)。</p>
<h3 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h3><p>若要做的是分类任务，只需找一个单调可做函数将分类任务的真实标记y与线性回归模型的预测值联系起来。</p>
<img src="/2020/02/10/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B1/单位阶跃函数与对数几率函数.png" style="zoom:100%;">

<p>上图中，左式是对数几率函数；右式是单位阶跃函数</p>
<p>从图3.2 可看出，单位阶跃函数不连续，因此不能直接使用，于是我们使用另外的单调可微的连续函数：对数几率函数。</p>
<p>对数几率函数是一种”Sigmoid 函数” (Sigmoid 函数即形似s的函数)，它将z值转化为一个接近0或1 的y值并且其输出值在z=0 附近变化很陡.将对数几率函数作为g()使用于广义线性模型 g(y) = wx + b 中即可将分类标记与回归模型联系起来。</p>
<p>这个方法是在用线性回归模型的预测结果去逼近真实标记的对数几率，因此，其对应的模型称为”对数几率回归”，虽然它的名字是”回归”，但实际是一种分类学习方法。</p>
<p>在其求解中，我们可通过”极大似然法” (maximum likelihood method)来估计ω和b。即令每个样本属于其真实标记的概率越大越好。最终得出的式子是关于β 的高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如梯度下降法(gradient descent method) 、牛顿法(Newton method)等都可求得其最优解。</p>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>有一可微分的函数。这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的梯度 ，然后朝着梯度相反的方向，就能让函数值下降的最快。因为梯度的方向就是函数之变化最快的方向。所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是测量下降方向的手段。</p>
<p>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率；在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向。</p>
<a id="more"></a>

<img src="/2020/02/10/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B1/梯度下降例子.png" style="zoom:90%;">

<p>如上，梯度就是分别对每个变量进行微分，梯度是一个向量。 </p>
<h5 id="模拟梯度下降的python代码"><a href="#模拟梯度下降的python代码" class="headerlink" title="模拟梯度下降的python代码"></a>模拟梯度下降的python代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#python3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集大小 即20个数据点</span></span><br><span class="line">m = <span class="number">20</span></span><br><span class="line"><span class="comment"># x的坐标以及对应的矩阵</span></span><br><span class="line">X0 = ones((m, <span class="number">1</span>))  <span class="comment"># 生成一个m行1列的向量，也就是x0，全是1</span></span><br><span class="line">X1 = arange(<span class="number">1</span>, m+<span class="number">1</span>).reshape(m, <span class="number">1</span>)  <span class="comment"># 生成一个m行1列的向量，也就是x1，从1到m</span></span><br><span class="line">X = hstack((X0, X1))  <span class="comment"># 按照列堆叠形成数组，其实就是样本数据</span></span><br><span class="line"><span class="comment"># 对应的y坐标</span></span><br><span class="line">Y = array([</span><br><span class="line">    <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">11</span>, <span class="number">8</span>, <span class="number">12</span>,</span><br><span class="line">    <span class="number">11</span>, <span class="number">13</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">21</span></span><br><span class="line">]).reshape(m, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(theta, X, Y)</span>:</span></span><br><span class="line">    diff = dot(X, theta) - Y  <span class="comment"># dot() 数组需要像矩阵那样相乘，就需要用到dot()</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span>/(<span class="number">2</span>*m)) * dot(diff.transpose(), diff)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义代价函数对应的梯度函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_function</span><span class="params">(theta, X, Y)</span>:</span></span><br><span class="line">    diff = dot(X, theta) - Y</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span>/m) * dot(X.transpose(), diff)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(X, Y, alpha)</span>:</span></span><br><span class="line">    theta = array([<span class="number">1</span>, <span class="number">1</span>]).reshape(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    gradient = gradient_function(theta, X, Y)</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> all(abs(gradient) &lt;= <span class="number">1e-5</span>):</span><br><span class="line">        theta = theta - alpha * gradient</span><br><span class="line">        gradient = gradient_function(theta, X, Y)</span><br><span class="line">    <span class="comment">#当梯度小于1e-5时，说明已经进入了比较平滑的状态，类似于山谷的状态,</span></span><br><span class="line">    <span class="comment">#这时候再继续迭代效果也不大了，所以这个时候可以退出循环</span></span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line">optimal = gradient_descent(X, Y, alpha)</span><br><span class="line">print(<span class="string">'optimal:'</span>, optimal)</span><br><span class="line">print(<span class="string">'cost function:'</span>, cost_function(optimal, X, Y)[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据数据画出对应的图像</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(X, Y, theta)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    ax = plt.subplot(<span class="number">111</span>)  <span class="comment">#</span></span><br><span class="line">    ax.scatter(X, Y, s=<span class="number">30</span>, c=<span class="string">"red"</span>, marker=<span class="string">"s"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"X"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Y"</span>)</span><br><span class="line">    x = arange(<span class="number">0</span>, <span class="number">21</span>, <span class="number">0.2</span>)  <span class="comment"># x的范围</span></span><br><span class="line">    y = theta[<span class="number">0</span>] + theta[<span class="number">1</span>]*x</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot(X1, Y, optimal)</span><br></pre></td></tr></table></figure>



<h3 id="线性判别分析"><a href="#线性判别分析" class="headerlink" title="线性判别分析"></a>线性判别分析</h3><p>线性判别分析(Linear Discriminant Analysis，简称LDA)，其思想：给定训练样例集设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。</p>
<img src="/2020/02/10/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B1/线性判别分析二维示意图.png" style="zoom:100%;">

<p>LDA 的二维示意图：”+”、” “分别代表正例和反例，椭圆表示数据簇的外轮廓，虚线表示投影， 红色实心园和实心三角形分别表示两类样本投影后的中心点.</p>
<p>欲使同类样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小;而欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大。</p>
<p>另J=类中心之间的距离/同类样例投影点的协方差，另J最大化。利用拉格朗日乘子法、奇异值分解等操作，最终即可求解。</p>
<p>推广至N维空间的情况，若将样本投影到N-1维空间，则可以进行降维，且投影过程中使用了类别信息，因此LDA也常被视为一种经典的监督降维技术。</p>
<h4 id="拉格朗日乘数法"><a href="#拉格朗日乘数法" class="headerlink" title="拉格朗日乘数法"></a>拉格朗日乘数法</h4><img src="/2020/02/10/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B1/拉格朗日乘数法1.png" style="zoom:70%;">

<p>假设有自变量x和y，给定约束条件g(x,y)=c，要求f(x,y)在约束g下的极值。可以画出f的等高线图，如上图。此时，约束g=c由于只有一个自由度，因此也是图中的一条曲线（红色曲线所示）。当约束曲线g=c与某一条等高线f=d1相切时，函数f取得极值。两曲线相切等价于两曲线在切点处拥有共线的法向量。因此可得函数f(x,y)与g(x,y)在切点处的梯度（gradient）成正比。</p>
<p>因为两条曲线相切，意味着他们在这点的法线平行，也就是法向量只差一个任意的常数乘子。 </p>
<p>于是我们便可以列出方程组求解切点的坐标(x,y)，进而得到函数f的极值。</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>相机运动方向检测1</title>
    <url>/2020/02/08/%E7%9B%B8%E6%9C%BA%E8%BF%90%E5%8A%A8%E6%96%B9%E5%90%91%E6%A3%80%E6%B5%8B1/</url>
    <content><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>代码实现相机运动方向的检测，用于后面的相机运动补偿。<br>目前还未完成，本篇仅为今日的小结</p>
<h2 id="算法大致思路"><a href="#算法大致思路" class="headerlink" title="算法大致思路"></a>算法大致思路</h2><p>该算法思路仅为当前思路，后续还会继续改进</p>
<p>对于每一帧：<br>1、提取其Surf特征点<br>2、与上一帧的特征点进行匹配，这里可以使用FlannBased与BruteForceMatcher，FlannBased更快而BruteForceMatcher更精确；考虑到视频处理的实时性，本人目前使用FlannBased<br>3、基于距离筛除一些不太可能的匹配，此步骤的前提是场景不会发生形变<br>4、根据已经匹配的特征点及其移动距离与方向，从而获取当前相机在此参考系下的移动情况</p>
<h3 id="主要使用到数据结构与对象"><a href="#主要使用到数据结构与对象" class="headerlink" title="主要使用到数据结构与对象"></a>主要使用到数据结构与对象</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">cv::Ptr&lt;SurfFeatureDetector&gt; detector;<span class="comment">//检测器</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;cv::KeyPoint&gt; key_points_1, key_points_2;<span class="comment">//提取的特征点序列</span></span><br><span class="line">cv::Mat descriptors1, descriptors2;<span class="comment">//特征描述子</span></span><br><span class="line">cv::Ptr&lt;cv::DescriptorMatcher&gt; matcher = </span><br><span class="line">    cv::DescriptorMatcher::create(<span class="string">"FlannBased"</span>);<span class="comment">//FlannBased匹配</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;cv::DMatch&gt; dmatches;<span class="comment">//匹配序列</span></span><br></pre></td></tr></table></figure>

<h3 id="目前进度"><a href="#目前进度" class="headerlink" title="目前进度"></a>目前进度</h3><img src="/2020/02/08/%E7%9B%B8%E6%9C%BA%E8%BF%90%E5%8A%A8%E6%96%B9%E5%90%91%E6%A3%80%E6%B5%8B1/match1.png" style="zoom:125%;">

<a id="more"></a>

<p>2020-02-08:</p>
<img src="/2020/02/08/%E7%9B%B8%E6%9C%BA%E8%BF%90%E5%8A%A8%E6%96%B9%E5%90%91%E6%A3%80%E6%B5%8B1/result.gif" style="zoom:125%;">

<p>2020-02-10:</p>
<p>优化了筛除误匹配算法的部分，更加减少了误匹配的情况：</p>
<img src="/2020/02/08/%E7%9B%B8%E6%9C%BA%E8%BF%90%E5%8A%A8%E6%96%B9%E5%90%91%E6%A3%80%E6%B5%8B1/result2.gif" style="zoom:125%;">

]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>机器视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>运动相机下的运动目标跟踪3</title>
    <url>/2020/02/07/%E8%BF%90%E5%8A%A8%E7%9B%B8%E6%9C%BA%E4%B8%8B%E7%9A%84%E8%BF%90%E5%8A%A8%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA3/</url>
    <content><![CDATA[<p>继上篇，以下为2月7日研究《Moving Objects Detection with a Moving Camera: A Comprehensive Review》的运动分割法部分的笔记</p>
<h3 id="Motion-segmentation-运动分割法2"><a href="#Motion-segmentation-运动分割法2" class="headerlink" title="Motion segmentation 运动分割法2"></a>Motion segmentation 运动分割法2</h3><p>《Background subtraction for moving cameras based on trajectory-controlled segmentation and label inference》(2015)提出了一个基于轨迹的分水岭分割算法。在应用一个双边的滤波器去柔化图像并增强edges后，无关梯度会被最小化，并且将轨迹点选为标签。这些标签将会用于watershed algorithm (分水岭算法,根据分水岭的构成来考虑图像的分割) 的seeds从而获得分割结果。在这个结果中，会根据轨迹的标签，将分割结果的相应标为前景与背景。最后，利用马尔可夫随机场算法(Markov Random Field，MRF)，让其中的能量函数最小化，使得用前景背景信息推断没有标签的部分的标签。</p>
<img src="/2020/02/07/%E8%BF%90%E5%8A%A8%E7%9B%B8%E6%9C%BA%E4%B8%8B%E7%9A%84%E8%BF%90%E5%8A%A8%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA3/P1_2015.png" style="zoom:75%;">

<p>上图中，红色点代表为检测出的背景部分，蓝色点代表检测出的前景部分；其中有一些由于噪点而误检测的特征点可以使用PCA算法筛除</p>
<p>《A multilayer-based framework for online back-ground subtraction with freely moving cameras》(2017)提出了Multi-Layer Background Subtraction(多层的背景差法)。他们使用了多标签的分割，而非二值标签的分割。每一个动作集群点都会被联系到一层中。在每一层中，像素的动作都会由Gaussian Belief Propagation (GaBP)(高斯置信度传播算法)评估。之后，由外观模型、先前的概率图、motion estimation(动作评估)去计算之后的概率图。多标签的分割是正是基于由MRF算法中最小化能量函数得出的概率图计算的。</p>
<img src="/2020/02/07/%E8%BF%90%E5%8A%A8%E7%9B%B8%E6%9C%BA%E4%B8%8B%E7%9A%84%E8%BF%90%E5%8A%A8%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA3/P2_2017.png" style="zoom:75%;">

<a id="more"></a>


<p>《Online background subtraction with freely moving cameras using different motion boundaries》(2018)基于光度和光流方向场，使用Canny detector来计算动作的边界 。他们还利用了对下一帧的前景位置预测，从而防止了不可靠的光度与方向的前景流场(foreground flow field)。</p>
<p>《Moving object segmentation using depth and optical flow in car driving sequences》(2016)使用了三种不同的聚类方法来分割三维的动作情况，从而获得前景背景的二值掩码。这三种方法是：simple k-means clustering（k-means聚类算法),spectral clustering(谱聚类算法) with a 4-connected graph,spectral clustering with fully connected graph。<br>k-means clustering:基于欧式距离来划分不同的类<br>spectral clustering: 将带权无向图划分为两个或两个以上的最优子图，使子图内部尽量相似，而子图间距离尽量距离较远 </p>
<p>《An efficient optical flow based motion detection method for non-stationary scenes》(2019)提出了一种双判断的机制来区分前景物体与背景。前景是由前景与背景之间的阈值差别+FlowNet2.0一起共同判断出来的。</p>
<h2 id="论文来源"><a href="#论文来源" class="headerlink" title="论文来源"></a>论文来源</h2><p>论文来源：《Moving Objects Detection with a Moving Camera: A Comprehensive Review》<br>Marie-Neige Chapela, Thierry Bouwmansb<br>aLab. L3I, LRUniv., Avenue Albert Einstein, 17000 La Rochelle, France<br>bLab. MIA, LRUniv., Avenue Albert Einstein, 17000 La Rochelle, France<br>[cs.CV] 15 Jan 2020</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>机器视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>运动相机下的运动目标跟踪2</title>
    <url>/2020/02/05/%E8%BF%90%E5%8A%A8%E7%9B%B8%E6%9C%BA%E4%B8%8B%E7%9A%84%E8%BF%90%E5%8A%A8%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA2/</url>
    <content><![CDATA[<p>以下为阅读《Moving Objects Detection with a Moving Camera: A Comprehensive Review》随读笔记</p>
<h3 id="Motion-segmentation-运动分割法"><a href="#Motion-segmentation-运动分割法" class="headerlink" title="Motion segmentation 运动分割法"></a>Motion segmentation 运动分割法</h3><p>运动分割法的大体思路为利用特征点的轨迹来分割每一帧，分成静态背景与移动物体。</p>
<img src="/2020/02/05/%E8%BF%90%E5%8A%A8%E7%9B%B8%E6%9C%BA%E4%B8%8B%E7%9A%84%E8%BF%90%E5%8A%A8%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA2/运动分割法example.png" style="zoom:125%;">

<p>《Background subtraction for moving cameras based on trajectory-controlled segmentation and label inference》(oct 2015)提出：根据特征点的轨迹相似度，并利用PCA算法(主成分分析算法)筛除false trajectories，从而获取几类集群特征点。</p>
<p>《a probabilistic model for causal motion segmentation in moving camera videos》(2016)使用了稠密光流差、旋转流差，从而获得了物体的移动流。然后由平移流(translational flow)估计角度场，并根据每个平移流的平移大小作为该平移角度的可靠性指标，然后由符合条件的流动角度的可能性评估每个像素的流动方向。最后，用贝叶斯公式(Bayes’ rule)获得每个像素的次可能移动，然后会被用于最后的分割当中。作者还提出了，利用一个修正的RANSAC算法选择3个超像素，然后用于去分割视频的第一帧，从而用于后续的估计背景的移动以抵消相机的晃动。</p>
<a id="more"></a>

<p>《Moving object segmentation using depth and optical flow in car driving sequences》(2016)通过使用动作消失点从2D视角动作中获取3D动作，并得出背景的深度。</p>
<p>《A multilayer-based framework for online back-ground subtraction with freely moving cameras》(2017)提出了第一集群轨迹，它动态地基于集群对各个轨迹的标签的异同。通过计算集群内的变化点，各个集群可以分别对应各个前景物体。</p>
<p>《Online background subtraction with freely moving cameras using different motion boundaries》(2018)的作者提出基于光度和光流方向场，利用Canny detector来计算动作的边界，从而获取初始 seeds 。其中，前景seeds是动作边缘上的点，而背景seeds是前景的动作检测方框上的点。</p>
<p>《An efficient optical flow based motion detection method for non-stationary scenes》(2019)基于FlowNet2.0得到稠密光流图。背景的光流是由Constrained RANSAC Algorithm(CRA)的二次变换函数计算出的。CRA是一种改进版本的RANSAC算法，它避免了过拟合并且改进了搜索效率。</p>
<p>《Flownet2.0: Evolution of optical flow estimation with deep networks》(2017)：FlowNet2.0是一种基于深度学习的光流预估算法。</p>
<h2 id="论文来源"><a href="#论文来源" class="headerlink" title="论文来源"></a>论文来源</h2><p>论文来源：《Moving Objects Detection with a Moving Camera: A Comprehensive Review》<br>Marie-Neige Chapela, Thierry Bouwmansb<br>aLab. L3I, LRUniv., Avenue Albert Einstein, 17000 La Rochelle, France<br>bLab. MIA, LRUniv., Avenue Albert Einstein, 17000 La Rochelle, France<br>[cs.CV] 15 Jan 2020</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>机器视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>运动相机下的运动目标跟踪1</title>
    <url>/2020/02/03/%E8%BF%90%E5%8A%A8%E7%9B%B8%E6%9C%BA%E4%B8%8B%E7%9A%84%E8%BF%90%E5%8A%A8%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA1/</url>
    <content><![CDATA[<h1 id="Moving-Objects-Detection-and-Tracking-with-a-Moving-Camera"><a href="#Moving-Objects-Detection-and-Tracking-with-a-Moving-Camera" class="headerlink" title="Moving Objects Detection and Tracking with a Moving Camera"></a>Moving Objects Detection and Tracking with a Moving Camera</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在本块内容中，将针对移动相机的移动目标检测跟踪进行研究与实现。</p>
<h2 id="方法概述"><a href="#方法概述" class="headerlink" title="方法概述"></a>方法概述</h2><p>由学者Marie-Neige Chapela, Thierry Bouwmansb发表，在 15 Jan 2020 被收录的论文《Moving Objects Detection with a Moving Camera: A Comprehensive Review》中，将运动相机的运动目标检测方法大致分为两类：一类称为one Plane类，将背景视为flat scenes来处理；第二类将背景作为several parts来处理。</p>
<h3 id="one-Plane作为单平面处理类"><a href="#one-Plane作为单平面处理类" class="headerlink" title="one Plane作为单平面处理类"></a>one Plane作为单平面处理类</h3><h4 id="Panoramic-background-subtraction-全背景减法"><a href="#Panoramic-background-subtraction-全背景减法" class="headerlink" title="Panoramic background subtraction 全背景减法"></a>Panoramic background subtraction 全背景减法</h4><p>一个移动的照相机捕捉到的图像可以被缝在一起形成一个较大的图像，即所谓的全景图或镶嵌图，从而可以再次作为一个静态相机处理。</p>
<h4 id="多相机法"><a href="#多相机法" class="headerlink" title="多相机法"></a>多相机法</h4><p>一些方法使用双摄像头系统，而不是构建全景图，两个摄像机使得具有广泛的视角来观察整体现场。</p>
<h4 id="Motion-compensation-运动补偿法"><a href="#Motion-compensation-运动补偿法" class="headerlink" title="Motion compensation 运动补偿法"></a>Motion compensation 运动补偿法</h4><p>补偿相机的运动，使得能使用应对静态相机的方法</p>
<h4 id="Subspace-segmentation-子空间分割法"><a href="#Subspace-segmentation-子空间分割法" class="headerlink" title="Subspace segmentation 子空间分割法"></a>Subspace segmentation 子空间分割法</h4><p>利用特征点的轨迹用来分离背景和前景。</p>
<h4 id="Motion-segmentation-运动分割法"><a href="#Motion-segmentation-运动分割法" class="headerlink" title="Motion segmentation 运动分割法"></a>Motion segmentation 运动分割法</h4><p>与子空间分割法类似，利用特征点的轨迹，将视频的每一帧分割成静态的背景或移动的物体，但不使用子空间</p>
<h3 id="several-parts方法类"><a href="#several-parts方法类" class="headerlink" title="several parts方法类"></a>several parts方法类</h3><h4 id="平面处理-视差处理"><a href="#平面处理-视差处理" class="headerlink" title="平面处理+视差处理"></a>平面处理+视差处理</h4><p>平面+视差分解法，是以场景为中心的方法；对主要的平面进行运动补偿</p>
<h4 id="Multi-planes-scene-representation-多平面的场景表示"><a href="#Multi-planes-scene-representation-多平面的场景表示" class="headerlink" title="Multi planes scene representation 多平面的场景表示"></a>Multi planes scene representation 多平面的场景表示</h4><p>利用RANSAC级联器算法区分各个平面后处理</p>
<h4 id="网格分割图像法"><a href="#网格分割图像法" class="headerlink" title="网格分割图像法"></a>网格分割图像法</h4><p>利用网格分割图像后进行处理</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>机器视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>My First Blog in here</title>
    <url>/2020/01/01/FirstBlog/</url>
    <content><![CDATA[<p>欢迎来到吴泽鑫的博客！</p>
<h2 id="About-me"><a href="#About-me" class="headerlink" title="About me"></a>About me</h2><h3 id="代码仓库"><a href="#代码仓库" class="headerlink" title="代码仓库"></a>代码仓库</h3><h4 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a>GitHub</h4><p> <a href="https://github.com/TheXeme" target="_blank" rel="noopener">https://github.com/TheXeme</a></p>
<h3 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h3><h4 id="GithubBlog"><a href="#GithubBlog" class="headerlink" title="GithubBlog"></a>GithubBlog</h4><p> <a href="https://thexeme.github.io/">https://thexeme.github.io/</a></p>
<h4 id="GiteeBlog"><a href="#GiteeBlog" class="headerlink" title="GiteeBlog"></a>GiteeBlog</h4><p><a href="https://thexeme.gitee.io/" target="_blank" rel="noopener">https://thexeme.gitee.io/</a></p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
</search>
