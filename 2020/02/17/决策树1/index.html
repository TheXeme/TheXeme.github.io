<!DOCTYPE html>
<html lang="en">

<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8" />
    
  <meta name="description" content="des 吴泽鑫的博客" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    决策树1 |  Xeme&#39;s Blog
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/images/logo/x.png" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

<link rel="alternate" href="/atom.xml" title="Xeme's Blog" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

</html>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-决策树1" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  决策树1
</h1>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/" class="article-date">
  <time datetime="2020-02-17T08:36:30.000Z" itemprop="datePublished">2020-02-17</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a>
  </div>

      
      
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">3.3k字</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">11分钟</span>
        </span>
    </span>
</div>

      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h2><p>一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点;叶结点对应于决策结果(底部决策完毕)，其他每个结点则对应于一个属性测试;每个结点包含的样本集合根据属性测试的结果被划分到子结点中(子决策);根结点包含样本全集.从根结点到每个叶结点的路径对应了一个判定测试序列.决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简单且直观的”分而治之” (divide-and-conquer) 策略。</p>
<p>决策树的生成是一个递归过程.在决策树基本算法中，有三种情形会导致递归返回:<br>(1) 当前结点包含的样本全属于同一类别，无需划分(已经可以得出是正样本还是负样本);<br>(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分;<br>(3) 当前结点包含的样本集合为空，不能划分.</p>
<p>在第(2)种情形下，我们把当前结点标记为叶结点，井将其类别设定为该结点所含样本最多的类别;<br>在第(3) 种情形下，同样把当前结点标记为叶结点，但将其类别设定为其父结点所含样本最多的类别.<br>注意这两种情形的处理实质不同:情形(2)是在利用当前结点的后验分布，而情形(3)则是把父结点的样本分布作为当前结点的先验分布.</p>
<h2 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h2><p>决策树学习的关键是如何选择最优划分属性。</p>
<h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>“信息熵” (information entropy)是度量样本集合纯度最常用的一种指标.假定当前样本集合D 中第k类样本所占的比例为Pk (k = 1, 2,. . . , IYI) ，则D的信息熵定义为:(值越小，则D的纯度越高)(在好瓜坏瓜分类中，可以理解为好瓜比例远高于坏瓜比例，或者反过来，因为通过求导可以知道，就是要使得样本比例中某一部分远高于其他；依此，用信息熵最小，就可以知道挑出某样本的“可行率”,若信息熵值越高，则越混乱(平均)，纯度越低)</p>
<img src="/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/信息熵.png" style="zoom:100%;">



<p>假定离散属性a有V个可能的取值，给分支结点赋予权重IDVI / IDI ，即样本数越多的分支结点的影响越大，于是可计算出用属性a对样本集D进行划分所获得的”信息增益” (information gain)</p>
<img src="/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/信息增益.png" style="zoom:75%;">

<p>一般而言，信息增益越大(子类的信息熵更小，因此说明这个分类器更能挑)，则意味着使周属性a来进行划分所获得的”纯度提升”越大.因此，我们可用信息增益来进行决策树的划分属性选择。ID3 决策树学习算法(Iterative Dichotomiser 迭代二分器)就是以信息增益为准则来选择划分属性。</p>
<p>通过在每个叶子节点计算使用剩余的所有可用属性会在此处的信息增益，挑出最高的作为此叶子节点的判别属性，就可以获得决策树(ID3决策树)</p>
<h3 id="增益率"><a href="#增益率" class="headerlink" title="增益率"></a>增益率</h3><p>但是信息增益准则也有坏处，信息增益准则对可取值数目较多的属性有所偏好。比如，将所有的样本单独分为一分支，这样每个节点仅有一个样本，分支节点纯度已达最大。然而，这样的决策树显然不具有泛化能力，无法对新样本进行有效预测.</p>
<p>为减少这种偏好可能带来的不利影响，著名的 C4.5 决策树算法不直接使用信息增益，而是使用”增益率” (gain ratio) 来选择最优划分属性.采用与式(4.2) 相同的符号表示，增益率定义为：</p>
<img src="/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/增益率.png" style="zoom:100%;">

<p>其中:属性a的”固有值”:属性a的可能取值数目越多(即V越大)，则IV(a) 的值通常会越大.</p>
<img src="/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/固有值.png" style="zoom:100%;">

<p>由于增益率准则对可取值数目较少的属性有所偏好，因此 C4.5决策树算法不直接使用增益率，而是 : 先从候选划分属性中找出信息增益高于平均水平的属性，再从<br>中选择增益率最高的.</p>
<h3 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h3><p>CART 决策树使用”基尼指数” (Gini index)来选择划分属性.数据集D的纯度可用基尼值来度量:</p>
<img src="/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/基尼值.png" style="zoom:100%;">

<p>直观来说， Gini(D) 反映了从数据集D中随机抽取两个样本其类别标记不一致的概率.因此，Gini(D) 越小，则数据集D 的纯度越高.</p>
<p>属性α 的基尼指数定义为：</p>
<img src="/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/基尼指数.png" style="zoom:100%;">

<p>于是，CART 决策树在候选属性集合A中，选择那个使得划分后基尼指数最小的属性作为最优划分属性</p>
<a id="more"></a>

<h2 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h2><p>剪枝(pruning)是决策树学习算法对付”过拟合”的主要手段.在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得”太好”了，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合.因此，可通过主动去掉一些分支来降低过拟合的风险.</p>
<p>决策树剪枝的基本策略有”预剪枝” (prepruning)和”后剪枝”(post”pruning) [Quinlan, 1993].<br>预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点;<br>后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点.</p>
<p>注：常见的性能评估的方法<br>1、留出法<br>2、交叉验证法<br>3、自助法</p>
<h3 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h3><p>预页剪枝使得决策树的很多分支都没有”展开“，这降低了过拟合的风险，也显著减少了决策树训练的时间开销和测试时间开销，.但另一方面，有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显著提高;预剪枝基于”贪心”本质禁止这些分支展开给预剪枝决策树带来了欠拟含的风险。</p>
<h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h3><p>后剪枝决策树通常比预剪枝决策树保留了更多的分支. 一般情形下?后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树.但后剪枝过程是在生成完全决策树之后进行的,并且要白底向上地对树中的所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多.</p>
<h2 id="连续与缺失"><a href="#连续与缺失" class="headerlink" title="连续与缺失"></a>连续与缺失</h2><h3 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a>连续值处理</h3><p>下面讨论如何在决策树学习中使用连续属性.</p>
<p>最简单的策略是采用二分法(bi-partition)对连续属性进行处理，这是C4.5决策树算法中采用的机制.</p>
<p>也可以尝试多个位置进行二划分，对每种划分计算其二分后的信息增益，选择能使得信息增益最大的二分点：(这正是基于信息增益划分而改来的)</p>
<img src="/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/连续值处理的信息增益.png" style="zoom:80%;">

<p>其中Gain(D, a, t) 是样本集D基于划分点t二分后的信息增益. 于是，我们就可选择使Gain(D, a, t) 最大化的划分点.</p>
<p>需注意的是，与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性.</p>
<p>例如在父结点上使用了”密度&lt;0.381” ，不会禁止在子结点上使用”密度&gt;0.294” .</p>
<h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3><p>现实任务中常会遇到不完整样本，即样本的某些属性值缺失.如果简单地放弃不完整样本，仅使用无缺失值的样本来进行学习，是对数据信息极大的浪费.</p>
<p>我们需解决两个问题:<br>(1) 如何在属性值缺失的情况下进行划分属性选择?(训练)<br>(2) 给定划分属性,若样本在该属性上的值缺失，如何对样本进行划分?(使用)</p>
<p>定义：对属性a，ρ表示无缺失值样本所占的比例，Pk表示无缺失值样本中第k 类所占的比例，Rv 则表示无缺失值样本中在属性a上取值v的样本所占的比例。基于上述定义，我们可将信息增益的计算式推广为:</p>
<img src="/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/缺失值信息增益.png" style="zoom:80%;">

<p>Ent信息熵与之前的上面式子相同:</p>
<img src="/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/信息熵.png" style="zoom:100%;">

<p>对问题(2) ，若样本x在划分属性a上的取值己知,则将x划入与其取值对应的子结点，且样本权值在于结点中保持为ωx.<br>若样本x在划分属性α 上的取值未知，则将x同时划入所有子结点，且样本权值在与属性值av对应的子结点中调整为Rv*ωx ，直观地看，这就是让同一个样本以不同的概率划入到不同的子结点中去.</p>
<p>这就是C4.5决策树算法使用的解决方案。</p>
<h2 id="多变量决策树"><a href="#多变量决策树" class="headerlink" title="多变量决策树"></a>多变量决策树</h2><p>若我们把每个属性视为坐标空间中的一个坐标轴，则d个属性描述的样本就对应了d维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界.</p>
<p>分类边界的每一段都是与坐标轴平行的这样的分类边界使得学习结果有较好的可解释性，因为每一段划分都直接对应了某个属性取值.但在学习任务的真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似;此时的决策树会相当复杂，由于要进行大量的属性测试，预测时间开销会很大：</p>
<img src="/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/决策树分类边界.png" style="zoom:100%;">

<p>若能使用斜的划分边界，如下图中红色线段所示，则决策树模型将大为简化”多变量决策树” (multivariate decision tree) 就是能实现这样的”斜划分”甚至更复杂划分的决策树.以实现斜划分的多变量决策树为例，在此类决策树中，非叶结点不再是仅对某个属性，而是对属性的线性组合进行测试</p>
<img src="/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/多变量决策树分类边界.png" style="zoom:100%;">

<p>于是，与传统的”单变量决策树” (univariate decision tree) 不同，在多变量决策树的学习过程中，<br>不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。</p>
<p>例如生成这样的决策树：</p>
<img src="/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/多变量决策树1.png" style="zoom:100%;">

<p>其对应分类边界就会如下图所示：</p>
<img src="/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/多变量决策树2.png" style="zoom:100%;">

<p>关于线性分类器，前面有谈到，其最基础的思想即是使得均方误差最小化。</p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>决策树学习算法最著名的代表是ID3 [1979, 1986] 、C4.5 [1993] 和CART [1984]. </p>
<p>在信息增益、增益率、基尼指数之外，人们还设计了许多其他的准则用于决策树划分选择，然而有实验研究[1989]表明，这些准则虽然对决策树的尺寸有较大影响，但对泛化性能的影响很有限.</p>
<p>剪枝方法和程度对决策树泛化性能的影响相当显著，有实验研究表明[1989]，在数据带有噪声时通过剪枝甚至可将决策树的泛化性能提高25%.</p>
<p>多变量决策树算法主要有OC1 [1994] 和[Brodley and Utgoff,1995] 提出的一系列算法.OC1 先贪心地寻找每个属性的最优权值，在局部优化的基础上再对分类边界进行随机扰动以试图找到更好的边界; [Brodley and Utgoff, 1995] 则直接引入了线性分类器学习的最小二乘法，还有一些算法试图在决策树的叶结点上嵌入神经网络，以结合这两种学习机制的优势，例如”感知机树” (Perceptron tree) [Utgoff, 1989b] 在决策树的每个叶结点上训练一个感知机，而[Guo and Gelfand, 1992] 则直接在叶结点上嵌入多层神经网络.</p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      <a data-url="https://thexeme.github.io/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/" data-id="ck6k65isi0000xgtx0fli56w2"
        class="article-share-link">分享</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>

    </footer>

  </div>

  
  
  <nav class="article-nav">
    
    
      <a href="/2020/02/13/%E7%9B%B8%E6%9C%BA%E8%BF%90%E5%8A%A8%E6%96%B9%E5%90%91%E6%A3%80%E6%B5%8B2/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">相机运动方向检测2</div>
      </a>
    
  </nav>


  

  

  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2019-2020
        TheXeme
      </li>
      <li>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <ul class="list-inline">
  <li>PV:<span id="busuanzi_value_page_pv"></span></li>
  <li>UV:<span id="busuanzi_value_site_uv"></span></li>
</ul>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    
    <aside class="sidebar">
      
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/logo/xeme.png" alt="Xeme&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>谢谢，但是暂时无需打赏~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/pay/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/pay/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
			onClick: (e) => {
      	document.getElementById(e.target.innerText).scrollIntoView()
      	return false;
    	}
    });
  </script>


<script>
  var ayerConfig = {
    mathjax: 
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">



<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  
  

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

</html>