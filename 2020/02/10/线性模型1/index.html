<!DOCTYPE html>
<html lang="en">

<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8" />
    
  <meta name="description" content="des 吴泽鑫的博客" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    线性模型1 |  Xeme&#39;s Blog
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/images/logo/x.png" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

<link rel="alternate" href="/atom.xml" title="Xeme's Blog" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

</html>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-线性模型1" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  线性模型1
</h1>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/02/10/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B1/" class="article-date">
  <time datetime="2020-02-10T08:55:02.000Z" itemprop="datePublished">2020-02-10</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a>
  </div>

      
      
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">2k字</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">7分钟</span>
        </span>
    </span>
</div>

      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="线性模型的基本形式"><a href="#线性模型的基本形式" class="headerlink" title="线性模型的基本形式"></a>线性模型的基本形式</h2><img src="/2020/02/10/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B1/基本形式.png" style="zoom:100%;">

<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>“线性回归” (linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记。通常使用均方误差，亦称平方损失，来作为性能度量，试图使均方误差最小。</p>
<p>基于均方误差最小化来进行模型求解的方法称为”最小二乘法” (least suare method). 在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小.(最小二乘法用途很广，不仅限于线性回归)</p>
<p>在求解使得均方差最小化的过程中，称为线性回归模型对最小二乘的参数估计。我们可对其均方差求导，另求导式为零，则可得w与b的最优解。</p>
<p>然而现实中，往往会求出多个解。这时选择哪一个作为解进行输出，则由算法的归纳偏好决定。常见的做法是引入正则化(regularization) 项.如，假设我们认为示例所对应的输出标记是在指数尺度上变化，则可以使得 lny = wx +b 。这就是”对数线性回归” (log-linear regression)。</p>
<p>更一般地，考虑单调可微函数g(.) ， 令 g(y) = wx + b , 这样得到的模型称为” 广义线性模型” (generalized linear model) ，其中函数g() 称为”联系函数” (link function)。</p>
<h3 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h3><p>若要做的是分类任务，只需找一个单调可做函数将分类任务的真实标记y与线性回归模型的预测值联系起来。</p>
<img src="/2020/02/10/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B1/单位阶跃函数与对数几率函数.png" style="zoom:100%;">

<p>上图中，左式是对数几率函数；右式是单位阶跃函数</p>
<p>从图3.2 可看出，单位阶跃函数不连续，因此不能直接使用，于是我们使用另外的单调可微的连续函数：对数几率函数。</p>
<p>对数几率函数是一种”Sigmoid 函数” (Sigmoid 函数即形似s的函数)，它将z值转化为一个接近0或1 的y值并且其输出值在z=0 附近变化很陡.将对数几率函数作为g()使用于广义线性模型 g(y) = wx + b 中即可将分类标记与回归模型联系起来。</p>
<p>这个方法是在用线性回归模型的预测结果去逼近真实标记的对数几率，因此，其对应的模型称为”对数几率回归”，虽然它的名字是”回归”，但实际是一种分类学习方法。</p>
<p>在其求解中，我们可通过”极大似然法” (maximum likelihood method)来估计ω和b。即令每个样本属于其真实标记的概率越大越好。最终得出的式子是关于β 的高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如梯度下降法(gradient descent method) 、牛顿法(Newton method)等都可求得其最优解。</p>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>有一可微分的函数。这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的梯度 ，然后朝着梯度相反的方向，就能让函数值下降的最快。因为梯度的方向就是函数之变化最快的方向。所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是测量下降方向的手段。</p>
<p>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率；在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向。</p>
<img src="/2020/02/10/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B1/梯度下降例子.png" style="zoom:90%;">

<p>如上，梯度就是分别对每个变量进行微分，梯度是一个向量。 </p>
<h5 id="模拟梯度下降的python代码"><a href="#模拟梯度下降的python代码" class="headerlink" title="模拟梯度下降的python代码"></a>模拟梯度下降的python代码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#python3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集大小 即20个数据点</span></span><br><span class="line">m = <span class="number">20</span></span><br><span class="line"><span class="comment"># x的坐标以及对应的矩阵</span></span><br><span class="line">X0 = ones((m, <span class="number">1</span>))  <span class="comment"># 生成一个m行1列的向量，也就是x0，全是1</span></span><br><span class="line">X1 = arange(<span class="number">1</span>, m+<span class="number">1</span>).reshape(m, <span class="number">1</span>)  <span class="comment"># 生成一个m行1列的向量，也就是x1，从1到m</span></span><br><span class="line">X = hstack((X0, X1))  <span class="comment"># 按照列堆叠形成数组，其实就是样本数据</span></span><br><span class="line"><span class="comment"># 对应的y坐标</span></span><br><span class="line">Y = array([</span><br><span class="line">    <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">11</span>, <span class="number">8</span>, <span class="number">12</span>,</span><br><span class="line">    <span class="number">11</span>, <span class="number">13</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">21</span></span><br><span class="line">]).reshape(m, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(theta, X, Y)</span>:</span></span><br><span class="line">    diff = dot(X, theta) - Y  <span class="comment"># dot() 数组需要像矩阵那样相乘，就需要用到dot()</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span>/(<span class="number">2</span>*m)) * dot(diff.transpose(), diff)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义代价函数对应的梯度函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_function</span><span class="params">(theta, X, Y)</span>:</span></span><br><span class="line">    diff = dot(X, theta) - Y</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span>/m) * dot(X.transpose(), diff)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(X, Y, alpha)</span>:</span></span><br><span class="line">    theta = array([<span class="number">1</span>, <span class="number">1</span>]).reshape(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    gradient = gradient_function(theta, X, Y)</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> all(abs(gradient) &lt;= <span class="number">1e-5</span>):</span><br><span class="line">        theta = theta - alpha * gradient</span><br><span class="line">        gradient = gradient_function(theta, X, Y)</span><br><span class="line">    <span class="comment">#当梯度小于1e-5时，说明已经进入了比较平滑的状态，类似于山谷的状态,</span></span><br><span class="line">    <span class="comment">#这时候再继续迭代效果也不大了，所以这个时候可以退出循环</span></span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line">optimal = gradient_descent(X, Y, alpha)</span><br><span class="line">print(<span class="string">'optimal:'</span>, optimal)</span><br><span class="line">print(<span class="string">'cost function:'</span>, cost_function(optimal, X, Y)[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据数据画出对应的图像</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(X, Y, theta)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    ax = plt.subplot(<span class="number">111</span>)  <span class="comment">#</span></span><br><span class="line">    ax.scatter(X, Y, s=<span class="number">30</span>, c=<span class="string">"red"</span>, marker=<span class="string">"s"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"X"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Y"</span>)</span><br><span class="line">    x = arange(<span class="number">0</span>, <span class="number">21</span>, <span class="number">0.2</span>)  <span class="comment"># x的范围</span></span><br><span class="line">    y = theta[<span class="number">0</span>] + theta[<span class="number">1</span>]*x</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot(X1, Y, optimal)</span><br></pre></td></tr></table></figure>



<h3 id="线性判别分析"><a href="#线性判别分析" class="headerlink" title="线性判别分析"></a>线性判别分析</h3><p>线性判别分析(Linear Discriminant Analysis，简称LDA)，其思想：给定训练样例集设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。</p>
<img src="/2020/02/10/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B1/线性判别分析二维示意图.png" style="zoom:100%;">

<p>LDA 的二维示意图：”+”、” “分别代表正例和反例，椭圆表示数据簇的外轮廓，虚线表示投影， 红色实心园和实心三角形分别表示两类样本投影后的中心点.</p>
<p>欲使同类样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小;而欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大。</p>
<p>另J=类中心之间的距离/同类样例投影点的协方差，另J最大化。利用拉格朗日乘子法、奇异值分解等操作，最终即可求解。</p>
<p>推广至N维空间的情况，若将样本投影到N-1维空间，则可以进行降维，且投影过程中使用了类别信息，因此LDA也常被视为一种经典的监督降维技术。</p>
<h4 id="拉格朗日乘数法"><a href="#拉格朗日乘数法" class="headerlink" title="拉格朗日乘数法"></a>拉格朗日乘数法</h4><img src="/2020/02/10/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B1/拉格朗日乘数法1.png" style="zoom:80%;">

<p>假设有自变量x和y，给定约束条件g(x,y)=c，要求f(x,y)在约束g下的极值。可以画出f的等高线图，如上图。此时，约束g=c由于只有一个自由度，因此也是图中的一条曲线（红色曲线所示）。当约束曲线g=c与某一条等高线f=d1相切时，函数f取得极值。两曲线相切等价于两曲线在切点处拥有共线的法向量。因此可得函数f(x,y)与g(x,y)在切点处的梯度（gradient）成正比。</p>
<p>因为两条曲线相切，意味着他们在这点的法线平行，也就是法向量只差一个任意的常数乘子。 </p>
<p>于是我们便可以列出方程组求解切点的坐标(x,y)，进而得到函数f的极值。</p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      <a data-url="https://thexeme.github.io/2020/02/10/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B1/" data-id="ck6g00vdr0001ootx6nob4ivb"
        class="article-share-link">分享</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>

    </footer>

  </div>

  
  
  <nav class="article-nav">
    
    
      <a href="/2020/02/08/%E7%9B%B8%E6%9C%BA%E8%BF%90%E5%8A%A8%E6%96%B9%E5%90%91%E6%A3%80%E6%B5%8B1/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">相机运动方向检测1</div>
      </a>
    
  </nav>


  

  

  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2019-2020
        TheXeme
      </li>
      <li>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <ul class="list-inline">
  <li>PV:<span id="busuanzi_value_page_pv"></span></li>
  <li>UV:<span id="busuanzi_value_site_uv"></span></li>
</ul>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    
    <aside class="sidebar">
      
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/logo/xeme.png" alt="Xeme&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>谢谢，但是暂时无需打赏~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/pay/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/pay/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
			onClick: (e) => {
      	document.getElementById(e.target.innerText).scrollIntoView()
      	return false;
    	}
    });
  </script>


<script>
  var ayerConfig = {
    mathjax: 
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">



<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  
  

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

</html>