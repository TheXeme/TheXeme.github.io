<!DOCTYPE html>
<html lang="en">

<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8" />
    
  <meta name="description" content="des 吴泽鑫的博客" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    支持向量机 |  Xeme&#39;s Blog
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/images/logo/x.png" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

<link rel="alternate" href="/atom.xml" title="Xeme's Blog" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

</html>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-支持向量机" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  支持向量机
</h1>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" class="article-date">
  <time datetime="2020-02-25T05:00:00.000Z" itemprop="datePublished">2020-02-25</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a>
  </div>

      
      
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">4.3k字</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">15分钟</span>
        </span>
    </span>
</div>

      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <blockquote>
<p>由于内容比较多，所以写了比较长的时间，从22号断断续续写到25号才写完</p>
</blockquote>
<h2 id="间隔与支持向量"><a href="#间隔与支持向量" class="headerlink" title="间隔与支持向量"></a>间隔与支持向量</h2><p>给定训练样本集：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/训练样本集.png" style="zoom:100%;">

<p>分类学习最基本的想法就是基于训练、集D 在样本空间中找到一个划分超平面、将<br>不同类别的样本分开。</p>
<p>在样本空间中，划分超平面可通过如下线性方程来描述:</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/线性方程.png" style="zoom:100%;">

<p>其中w = (ω1,ω2, … ,ωd) 为法向量，决定了超平面的方向; b 为位移项，决定了超平面与原点之间的距离.显然，划分超平面可被法向量ω和位移b确定，下面我们将其记为(ω,b).样本空间中任意点x到超平面(ω,b)的距离可写为:</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/点到平面距离.png" style="zoom:100%;">

<p>证明如下：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/点到平面距离证明.jpg" style="zoom:50%;">

<p>假设超平面(ω,b)能将训练样本正确分类，即对于(xi,yi) ∈ D , 若yi = +1 , 则有 ωTxi+b&gt;0;若yi = -1 , 则有 ωTxi+b&lt;0。令：(ω和b参数的约束式)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/支持向量机基本分类式.png" style="zoom:100%;">

<p>如下图所示，距离超平面最近的这几个训练样本点使上式的等号成立，它们被称为”支持向量” (support vector) ，两个异类支持向量到超平面的距离之和为:</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/异类间隔.png" style="zoom:100%;">

<p>它被称为间隔。(使用上面的点到超平面距离公式代入上面正确分类的定义式即可很容易推导出)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/支持向量与间隔.png" style="zoom:100%;">

<p>欲找到具有”最大间隔” (maximum margin) 的划分超平面，也就是要找<br>到能满足式中约束的参数ω和b ， 使得γ最大，即:</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/约束式写法1.png" style="zoom:100%;">

<p>其中s.t. ，指 subject to，受限制于… ，即指代2式为约束，然后使得1式最大。之所以可以写成2式，是因为在负样本中，亦有： (-1)*(-1)=1&gt;0 </p>
<p>注：在此处，间隔看似仅与ω有关，但事实上b通过约束式暗中地影响着ω的取值，进而对间隔产生影响</p>
<p>重新看回上式为了最大化1式(最大化间隔)，只需使其分母最小，因此可以等价为：（6.6）</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/约束式写法2.png" style="zoom:100%;">

<p>这就是支持向量机(Support Vector Machine，简称SVM) 的基本型.</p>
<p>最后，超平面对应的模型为：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/超平面模型.png" style="zoom:100%;">

<a id="more"></a>

<h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><p>对上上式使用拉格朗日乘子法可得到其”对偶问题” (dual problem).</p>
<blockquote>
<p>可参考： <a href="https://blog.csdn.net/fkyyly/article/details/86488582" target="_blank" rel="noopener">https://blog.csdn.net/fkyyly/article/details/86488582</a> </p>
</blockquote>
<p>具体来说，对式中每条约束添加拉格朗日乘子 αi&gt;=0 ，则该问题的拉格朗日函数可写为:(式6.8)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/拉格朗日函数.png" style="zoom:100%;">

<p>其中α=(α1;α2; … ;αm). 令L(ω , b , α) 对ω和b的偏导为零可得:(6.9)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/求偏导等为零.png" style="zoom:100%;">

<p>再将这两式先后代回式6.8，即可将其中的ω和b消去，得到：(6.11)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式6111.png" style="zoom:100%;">

<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式6112.png" style="zoom:100%;">

<p>对此式，求解α(求解方法下面会说)，得出α后，再算出ω和b，即：(6.12)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/对偶最终模型.png" style="zoom:100%;">

<p>从对偶问题(6.11)解出的问是式(6.8) 中的拉格朗日乘子，它恰对应着训练样本(Xi,Yi). 由于式(6.6) 中有不等式约束，因此上述过程需满足KKT(Karush-Kuhn-Tucker) 条件，即要求：(6.13)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/KKT条件.png" style="zoom:100%;">

<p>于是，对任意训练样本(Xi， Yi) ， 总有αi=0 或 yf(Xi) = 1.<br>若αi=0 , 则该样本将不会在式(6.12) 的求和中出现，也就不会对f(x)有任何影响;<br>若αi&gt;0 , 则必有 yif(xi) = 1 ，其所对应的样本点位于最大间隔边界上，是一个支持向量.<br>这显示出支持向量机的一个重要性质:训练完成后，大部分的训练样本都不需保留，最终模型仅与支持向量有关，其复杂度主要与支持向量的数目有关。</p>
<p>回到上面求解6.11式的问题：<br>这是一个二次规划问题，可使用通用的二次规划算法来求解；然而，该问题的规模正比于训练样本数，这会在实际任务中造成很大的开销.为了避开这个障碍，人们通过利用问题本身的特性，提出了很多高效算法， SMO (Sequential Minimal Optimization) (顺序最小优化)是其中一个著名的代表[Platt 1998].</p>
<h3 id="SMO"><a href="#SMO" class="headerlink" title="SMO"></a>SMO</h3><p>SMO 的基本思路是先固定αi之外的所有参数，然后求αi上的极值.由于存在约束：(上6.9)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/SMO约束.png" style="zoom:100%;">

<p>若固定αi之外的其他变量，那么αi也可由其他变量求解出，于是， SMO 每次选择两个变量αi和αj，并固定其他参数，这样，在参数初始化后， SMO 不断执行如下两个步骤直至收敛:<br>(1)选取一对需更新的变量αi和αj;<br>(2)固定αi和αj以外的参数，求解式(6.11)获得更新后的αi和αj</p>
<p>注意到只需选取的αi和αj中有一个不满足KKT 条件(6.13) ，目标函数就会在选代后增长[Osuna et al., 1997]. 直观来看， KKT 条件违背的程度越大，则变量更新后可能导致的目标函数值增幅越多。</p>
<p>于是， SMO 先选取违背KKT条件程度最大的变量.第二个变量应选择一个使目标函数值增长最快的变量</p>
<blockquote>
<p>注：回看式子6.13：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/KKT条件.png" style="zoom:100%;">

<p>除了第一个非负约束以外，其他约束都是根据目标函数推导得到的最优解必须满足的条件，如果违背了这些条件，那得到的解必然不是最优的，目标函数的值会减小。 </p>
<p>所以在SMO迭代的两个步骤中，只要αi和αj中有一个违背了KKT条件，这一轮迭代完成后，目标函数的值必然会增大。Generally speaking，KKT条件违背的程度越大，迭代后的优化效果越明显，增幅越大。 </p>
<p>因此，若要使得整个算法更快速，和梯度下降类似，我们要找到使之优化程度最大的方向（变量）进行优化。</p>
<p>所以SMO先选取违背KKT条件程度最大的变量，那么第二个变量应该选择使目标函数值增大最快的变量，但是这个变量怎么找呢？比较各变量优化后对应的目标函数值的变化幅度？这个样子是不行的，复杂度太高了。 </p>
</blockquote>
<p>由于比较各变量所对应的目标函数值减幅的复杂度过高，因此SMO 采用了一个启发式(不能得到最优解，但能得到可行解，与最优解之间的误差在可接受范围内):使选取的两变量所对应样本之间的间隔最大.一种直观的解释是，这样的两个变量有很大的差别，与对两个相似的变量进行更新相比，对它们进行更新会带给目标函数值更大的变化.</p>
<p>SMO 算法之所以高效，是由于在固定其他参数后，仅优化两个参数的过程能做到非常高效.具体来说，仅考虑αi和αj时，式(6.11) 中的约束可重写为（将约束式中αiyi和αjyj提出即可得）(6.14)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式614.png" style="zoom:100%;">

<p>其中，c: (6.15)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式615.png" style="zoom:100%;">

<p>用上式代入6.11中原式1，即可消去变量αj，得到一个关于αi的单变量二次规划问题，仅有的约束是αi&gt;=0. (的二次规划问题具有公式解(解析解))</p>
<p>如何确定偏移项b呢?因为支持向量位于边界，即对任意支持向量(Xs,Ys) ,都有 Ysf(Xs) = 1, 即：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式617.png" style="zoom:100%;">

<p>其中S为所有支持向量的F标集.理论上，可选取任意支持向量并通过求上解式获得b；但现实任务中常采用一种更鲁棒的做法：使用所有支持向量求解的平均值：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式618.png" style="zoom:100%;">

<h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><h3 id="核函数引言"><a href="#核函数引言" class="headerlink" title="核函数引言"></a>核函数引言</h3><img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/核函数异或映射.png" style="zoom:100%;">

<p>利用函数(不叫核函数)，将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分.</p>
<p>令Φ(x) 表示将x映射后的特征向量，于是， 在特征空间中划分超平面所对应的模型可表示为:</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式619.png" style="zoom:100%;">

<p>其中ω和b是模型参数.类似式(6.6），有：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式620.png" style="zoom:100%;">

<p>其对偶问题为：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式6211.png" style="zoom:100%;">

<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式6212.png" style="zoom:100%;">

<h3 id="核技巧-kernel-trick"><a href="#核技巧-kernel-trick" class="headerlink" title="核技巧(kernel trick)"></a>核技巧(kernel trick)</h3><p>求解上式涉及到计算Φ(xi)TΦ(xj)， 这是样本Xi与Xj映射到特征空间之后的内积.由于特征空间维数可能很高，因此直接计算Φ(xi)TΦ(xj)通常是复杂困难的.为了避开这个障碍，可以设想这样一个函数:</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式622.png" style="zoom:100%;">

<p>即Xi与Xj在特征空间的内积等于它们在原始样本空间中通过函数κ(. , .)计算的结果.有了这样的函数,我们就不必直接去计算高维甚至无穷维特征空间中的内积,（这就称为核技巧），于是：(6.24)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式623.png" style="zoom:100%;">

<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式624.png" style="zoom:100%;">

<p>这里的函数κ(. ， .)就是”核函数” (kernel function) .模型最优解可通过训练样本的核函数展开，这一展式亦称”支持向量展式”(support vector expansion).</p>
<h3 id="核函数-1"><a href="#核函数-1" class="headerlink" title="核函数"></a>核函数</h3><p>令χ为输入空间，κ(.,.) 是定义在 χ*χ 上的对称函数，则κ是核函数当且仅当对于任意数据D = {x1,…,xm} ，”核矩阵” (kernel matrix) K 总是半正定的:</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/核矩阵.png" style="zoom:100%;">

<p>只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用.事实上，对于一个半正定核矩阵，总能找到一个与之对应的映射Φ.</p>
<p>我们希望样本在特征空间内线性可分，因此特征空间的好坏对支持向量机的性能至关重要.需注意的是，在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式地定义了这个特征空间.于是，”核函数选择”成为支持向量机的最大变数.若核函数选择不合适，则意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳.</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/常用核函数.png" style="zoom:100%;">

<p>此外，核函数还可通过多个函数组合得到</p>
<h2 id="软间隔与正则化"><a href="#软间隔与正则化" class="headerlink" title="软间隔与正则化"></a>软间隔与正则化</h2><h3 id="软间隔与硬间隔"><a href="#软间隔与硬间隔" class="headerlink" title="软间隔与硬间隔"></a>软间隔与硬间隔</h3><p>在现实任务中，往往很难确定合适的核函数使得训练样本在特征空间中线性可分，即使恰好找到了某个核函数使训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果不是由于过拟合所造成的.</p>
<p>缓解该问题的一个办法是允许支持向量机在一些样本上出错.为此，要引入”软间隔” (soft margin) 的概念:</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/软间隔.png" style="zoom:100%;">

<p>硬间隔 (hard margin) ：所有样本都必须划分正确(之前所推导的都是硬间隔)<br>软间隔：允许某些样本不满足约束(6.28)： (如上图中的红点)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式628.png" style="zoom:100%;">

<p>在最大化间隔的同时，不满足约束的样本应尽可能少。于是，优化目标可写为：(6.29)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式629.png" style="zoom:100%;">

<p>其中C&gt;0 是一个常数， l （lost01）是”0/1损失函数” :</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式630.png" style="zoom:100%;">

<blockquote>
<p>当C为无穷大时，式(6.29)迫使所有样本均满足约束(6.28) ，于是式(6.29) 等价于(6.6)(硬间隔);<br>当C取有限值时，式(6.29)允许一些样本不满足约束.</p>
</blockquote>
<p>然而上式l非凸、非连续，数学性质不太好，使得式(6.29)不易直接求解.于是，人们通常用其他一些函数来代替l(0/1) ， 称为”替代损失” (surrogate loss).替代损失函数一般具有较好的数学性质，如它们通常是凸的连续函数且是l(0/1)的上界.三种常用的替代损失函数:</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/替代损失函数.png" style="zoom:100%;">

<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/替代损失函数图.png" style="zoom:70%;">

<p>例如，若采用hinge 损失，则式(6.29)变成：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式634.png" style="zoom:100%;">

<p>引入”松弛变量” (slack variabies) ξi&gt;=0 可将式重写为:</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式635.png" style="zoom:100%;">

<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式6352.png" style="zoom:100%;">

<p>这就是常用的”软间隔支持向量机”</p>
<p>类似于前面，使用拉格朗日乘子法得到拉格朗日函数，令函数对ω、b、ξ(多了个ξ)求偏导并令为0，再代回式子，即得到对偶问题，然后用上面同样的SMO算法即可得到支持向量展开式</p>
<blockquote>
<p>由于使用软间隔支持向量机，其不要求所有样本都满足约束，因此其KKT条件：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/641.png" style="zoom:100%;">
</blockquote>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>我们还可以把式(6.29) 中的l(0/1)损失函数换成别的替代损失函数以得到其他学习模型，这些模型的性质与所用的替代函数直接相关，但它们具有一个共性:优化目标中的第一项用来描述划分超平面的”间隔”大小，后面一项用来表述训练集上的误差，可写为更一般的形式：(6.42)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/642.png" style="zoom:100%;">

<p>其中。第一项Ω(f) 称为”结构风险” (structural risk) ，用于描述模型f的某些性质(间隔);<br>第二项称为”经验风险” (empirical risk) ，用于描述模型与训练数据的契合程度;<br>C用于对二者进行折中.</p>
<p>从经验风险最小化的角度来看，Ω(f)表述了我们希望获得具有何种性质的模型(例如希望获得复杂度较小的模型)<br>另一方面，利用经验风险最小化，有助于削减假设空间，从而降低了最小化训练误差的过拟合风险.</p>
<p>从这个角度来说，式(6.42) 称为”正则化” (regularization) 问题，Ω(f) 称为正则化项， C则称为正则化常数.</p>
<blockquote>
<p>正则化可理解为一种”罚函数法”，即对不希望得到的结果采施以惩罚，从而使得优化过程趋向于希望目标.</p>
<p>Lp 范数(norm) 是常用的正则化项，<br>其中L2 范数IlωIl2 倾向于ω的分量取值尽量均衡，即非零分量个数尽量稠密<br>而Lo范数Ilwllo 和L1范数Ilwll1则倾向于ω 的分量尽量稀疏，即非零分量个数尽量少.</p>
</blockquote>
<h2 id="支持向量回归"><a href="#支持向量回归" class="headerlink" title="支持向量回归"></a>支持向量回归</h2><p><strong>支持向量回归(Support Vector Regression，SVR)</strong></p>
<p>前面讨论的是分类问题，下面讨论回归问题。</p>
<p>给定训练样本D = {(X1, Y1) , (X2，Y2),…,(Xm, Ym)}, 希望学得一个形如式(6.7) ：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/超平面模型.png" style="zoom:100%;">

<p>的回归模型，使得f(x)与y尽可能接近， ω 和b是待确定的模型参数.</p>
<p>对样本(X,Y)，传统回归模型通常直接基于模型输出f(x)与真实输出y之间的差别来计算损失，当且仅当f(x)与y完全相同时，损失才为零.</p>
<p>与此不同，支持向量回归(Support Vector Regression，简称SVR)假设我们能容忍f(x)与y之间最多有ε的偏差，即仅当f(x) 与y之间的差别绝对值大于ε时才计算损失.如下图所示，这相当于以f(x) 为中心，构建了一个宽度为2ε的间隔带，若训练样本落入此间隔带，则认为是被预测正确的.</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/支持向量回归.png" style="zoom:100%;">

<p>于是， SVR 问题可形式化为：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/643.png" style="zoom:100%;">

<p>其中C为正则化常数，l是ε不敏感损失(ε insensitive loss) 函数</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/644.png" style="zoom:100%;">

<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/不敏感损失函数.png" style="zoom:100%;">

<p>引入松弛变量ξ(可以有一些样本点不满足约束) (间隔带两侧的松弛程度可有所不同)，可将式重写为:</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/645.png" style="zoom:100%;">

<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/6452.png" style="zoom:100%;">

<p>其求解过程也是跟前面一样的：</p>
<p>引入拉格朗日乘子α1(上侧)、α2(下侧)、μ1、μ2，由拉格朗日乘子法：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/646.png" style="zoom:100%;">

<p>令拉格朗日函数对ω、b、ξ求偏导并为零：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/647.png" style="zoom:100%;">

<p>代回拉格朗日函数中，得到SVR的对偶问题：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/651.png" style="zoom:100%;">

<p>上述过程中需满足KKT条件：(6.52)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/652.png" style="zoom:100%;">

<p>可以看出，当且仅当f(Xi) -yi -ε-ξi = 0 时α能取非零值，2式同理，换言之，仅当样本(Xi ， Yi) 不落入ε间隔带中相应的α才能取非零值.</p>
<p>此外，由于约束f(Xi) -yi -ε-ξi = 0 、f(Xi) -yi -ε-ξi拔= 0 无法同时成立(因为不可能同时在上边界又在下边界)，因此两个α至少有一个为零</p>
<blockquote>
<p>落在ε-间隔带中的样本都满足αi = 0 且αi拔=0</p>
</blockquote>
<p>最终解得SVR解：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/653.png" style="zoom:100%;">

<p>能使式中的αi拔-αi ≠ 0的样本即为SVR的支持向量，它们必落在ε间隔带之外.显然，SVR的支持向量仅是训练样本的一部分</p>
<p>对b的求解：</p>
<p>由KKT 条件(6.52) 可看出，对每个样本(Xi，Yi) 都有 (C-αi)ξi=0 且 αi( f(xi) -yi -ε -ξi) =0 ，因此，在得到αi后，对任意一个样本的αi，若0&lt;αi&lt;C ，则必有ξi=0，进而有：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/654.png" style="zoom:100%;">

<p>因此，在求解到αi后，理论上可以使用任意一个满足0&lt;αi&lt;C的αi来对b进行求解。实际中，使用多个或所有的满足条件0&lt;αi&lt;C的αi对b进行求解然后求平均值</p>
<p>最后，对于ω，若再加以考虑核函数的映射，则对前面的拉格朗日求偏导得零处，可改为：</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/655.png" style="zoom:100%;">

<p>最后得SVR模型：(6.56)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/656.png" style="zoom:100%;">

<h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>分类问题：(SVM 6.24)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/式624.png" style="zoom:100%;">

<p>回归问题：(SVR 6.56)</p>
<img src="/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/656.png" style="zoom:100%;">








      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      <a data-url="https://thexeme.github.io/2020/02/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" data-id="ck71frfhq000058tx3pruclkj"
        class="article-share-link">分享</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>

    </footer>

  </div>

  
  
  <nav class="article-nav">
    
    
      <a href="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">神经网络</div>
      </a>
    
  </nav>


  

  

  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2019-2020
        TheXeme
      </li>
      <li>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <ul class="list-inline">
  <li>PV:<span id="busuanzi_value_page_pv"></span></li>
  <li>UV:<span id="busuanzi_value_site_uv"></span></li>
</ul>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    
    <aside class="sidebar">
      
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/logo/xeme.png" alt="Xeme&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>谢谢，但是暂时无需打赏~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/pay/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/pay/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
			onClick: (e) => {
      	document.getElementById(e.target.innerText).scrollIntoView()
      	return false;
    	}
    });
  </script>


<script>
  var ayerConfig = {
    mathjax: 
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">



<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  
  

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

</html>