<!DOCTYPE html>
<html lang="en">

<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8" />
    
  <meta name="description" content="des 吴泽鑫的博客" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    神经网络 |  Xeme&#39;s Blog
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/images/logo/x.png" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

<link rel="alternate" href="/atom.xml" title="Xeme's Blog" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

</html>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-神经网络" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  神经网络
</h1>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="article-date">
  <time datetime="2020-02-20T14:36:30.000Z" itemprop="datePublished">2020-02-20</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a>
  </div>

      
      
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">6.1k字</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">21分钟</span>
        </span>
    </span>
</div>

      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h2><img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/神经元模型.png" style="zoom:100%;">

<p>上图为”M-P 神经元模型” [McCulloch and Pitts, 1943] 。</p>
<p>神经元接收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接(connection)进行传递，神经元接收到的总输入值将与神经元的阀值进行比较，然后通过”激活函数” (activation function) 处理以产生神经元的输出.</p>
<p>理想中的激活函数是阶跃函数，但阶跃函数具有不连续、不光滑等不太好的性质，因此实际常用Sigmoid函数作为激活函数（有时也称为”挤压函数” (squashing function）)，对数几率函数时Sigmoid函数的代表。(见前面线性模型中谈到的对数几率回归)</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/激活函数.png" style="zoom:100%;">

<p>把许多个这样的神经元按一定的层次结构连接起来，就得到了神经网络.</p>
<h2 id="感知机与多层网络"><a href="#感知机与多层网络" class="headerlink" title="感知机与多层网络"></a>感知机与多层网络</h2><img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/感知机网络结构.png" style="zoom:100%;">

<p>y = f(Σωixi - θ) ;一般地，给定训练数据集权重ωi(i=1,2,…,n) 以及阔值θ。可通过学习得到.这样，权重和阈值的学习就可统一为权重的学习.感知机学习规则非常简单，对训练样例（x,y) ，若当前感知机的输出为y拔， 则感知机权重将这样调整:</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/感知机权重调整.png" style="zoom:100%;">

<p>其中 0&lt;n&lt;1 ，称为学习率(learning rate)。可看出，若感知机对训练样例(x,y) 预测正确，即y拔 = y ， 则感知机不发生变化，否则将根据错误的程度进行权重调整.</p>
<p>若两类模式是线性可分的，即存在一个线性超平面能将它们分开。”非线性可分”意味着用线性起平面无法划分。与或非问题时线性可分的，而异或是非线性可分的。在非线性可分问题中，单层感知机学习过程将会发生震荡(fluctuation) ,ω难以稳定下来，不能求得合适解，例如单层感知机不能解决异或这样简单的非线性可分问题.</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/与或非异或.png" style="zoom:100%;">

<p>要解决非线性可分问题，需考虑多层功能神经元. 例如下图中这个简单的两层感知机就能解决异或问题.  输出层与输入层之间的一层神经元，被称为隐层或隐含层(hidden layer) ，隐含层和输出层神经元都是拥有激活函数的功能神经元.</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/两层感知机.png" style="zoom:100%;">

<p>更一般的，常见的神经网络是形如下图所示的层级结构，每层神经元与下层神经元全互连，神经元之间不存在同层连接， 也不存在跨层连接. 这样的神经网络结构通常称为” 多层前馈神经网络” (multi-layer feedforward neural networks) ，（”前债”并不意味着网络中信号不能向后传，而是指网络拓扑结构上不存在环或回路）其中输入层神经元接收外界输入，隐层与输出层神经元对信号进行加工。输入层神经元仅是接受输入，不进行函数处理。</p>
<p>神经网络的学习过程，就是根据训练数据来调整神经元之间的”连接权” (connection weight) 以及每个功能神经元的阈值;换言之，神经网络”学”到的东西，蕴涵在连接权与阈值中。</p>
<h2 id="误差逆传播算法"><a href="#误差逆传播算法" class="headerlink" title="误差逆传播算法"></a>误差逆传播算法</h2><p>欲训练多层网络，需要更强大的学习算法，如误差逆传播(errorBackPropagation，简称BP)算法。BP 算法不仅可用于多层前馈神经网络，还可用于其他类型的神经网络例如训练递归神经网络[1987].</p>
<p>假定：拥有d 个输入神经元、l 个输出神经元、q 个隐层神经元的多层前馈网络结构，其中输出层第j个神经元的阀值用θj 表示，隐层第h个神经元的阔值用γh 表示.输入层第i个神经元与隐层第h个神经元之间的连接权为νih，隐层第h个神经元与输出层第j个神经元之间的连接权为ωhj. 记隐层第h个神经元接收到的输入为: </p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/隐含层接收.png" style="zoom:100%;">

<p>输出层第j个神经元接收到的输入为:(其中bh 为隐层第h个神经元的输出)</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/输出层接收.png" style="zoom:100%;">

<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/BP网络.png" style="zoom:100%;">

<p>网络中有(d+ l +1) q + l 个参数需确定:输入层到隐层的dxq个权值、隐层到输出层的q x l 个权值、q 个隐层神经元的阔值、l个输出层神经元的阔值. BP 是一个迭代学习算法，在迭代的每一轮中采用广义的感知机学习规则对参数进行更新估计</p>
<p>BP 算法基于梯度下降(gradient descent)策略， 以目标的负梯度方向对参数进行调整.</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/BP1.jpg" style="zoom:50%;">

<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/BP2.jpg" style="zoom:50%;">

<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/BP3.jpg" style="zoom:50%;">

<p>学习率 0&lt;η&lt;1 控制着每一轮迭代中的更新步长,太大则容易振荡，太小则收敛速度又会过慢，常设置为η=0.1 。 有时为了做精细调节，可使每层神经元使用不同的学习率。</p>
<p>BP算法的工作流程：对每个训练样例，BP 算法执行以下操作:<br>先将输入示例提供给输入层神经元，然后逐层将信号前传，直到产生输出层的结果;然后计算输出层的误差(第4-5行) ，再将误差逆向传播至隐层神经元(第6 行) ，最后根据隐层神经元的误差来别连接权和阈值进行调整(第7行).该法代过程循环进行，直到达到某些停止条件为止，比如误差已经很小，或迭代次数达到上限。</p>
<a id="more"></a>

<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/BP算法.png" style="zoom:100%;">

<p>上面介绍的”标准BP算法”每次仅针对一个训练样例更新连接权和阈值，即根据单个Ek推导每次的更新。</p>
<p>BP算法的最终目标是最小化整个训练集的累计误差：</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/累计误差.png" style="zoom:100%;">

<h3 id="累积BP算法"><a href="#累积BP算法" class="headerlink" title="累积BP算法"></a>累积BP算法</h3><p>类似于上面的标准BP算法，推导出基于累积误差最小化的更新规则，就得到了累积误差逆传播(accumulated error backpropagation)算法，累积BP算法与标准BP算法都很常用。</p>
<p>一般来说，标准BP算法每次更新只针对单个样例，参数更新得非常频繁，而且对不同样例进行更新的效果可能出现”抵消”现象.因此，为了达到同样的累积误差极小点，标准BP 算法往往需进行更多次数的法代.累积BP算法直接针对累积误差最小化，它在读取整个训练集D一遍后才对参数进行更新，(读取训练集一遍称为进行了”一轮” 学习)其参数更新的频率低得多.但在很多任务中，累积误差下降到一定程度之后，进一步下降会非常缓慢，这时标准BP往往会更快获得较好的解，尤其是在训练集D非常大时更明显.</p>
<h3 id="再谈梯度下降"><a href="#再谈梯度下降" class="headerlink" title="再谈梯度下降"></a>再谈梯度下降</h3><p>标准BP算法和累积BP算法的区别类似于随机梯度下降(stochastic gradient descent，简称SGD)与标准梯度下降之间的区别.随机梯度下降是每次迭代使用一个样本来对参数进行更新，使得训练速度加快。而批量梯度下降（Batch Gradient Descent ， BGD在每一次迭代时使用所有样本来进行梯度的更新。 从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目，但收敛速度更快。</p>
<h3 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h3><p>由于BP神经网络强大的表示能力， BP神经网络经常遭遇过拟合。有两种策略常用来缓解BP 网络的过拟合.<br>第一种策略是”早停” (early stopping): 将数据分成训练集和验证集，训练、集用来计算梯度、更新连接权和阔值，验证集用来估计误差，若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值.(这与前面提到的决策树剪枝有一点类似)<br>第二种策略是”正则化” (regularization)，其基本思想是在误差目标函数中增加一个用于描述网络复杂度的部分，例如连接权与阔值的平方和.仍令Ek 表示第k个训练样例上的误差，ωi表示连接权和阈值，则误差目标函数E改进为:</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/引入正则项的误差目标函数.png" style="zoom:100%;">

<p>其中 0&lt;λ&lt;1用于对经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计.</p>
<h2 id="全局最小与局部极小"><a href="#全局最小与局部极小" class="headerlink" title="全局最小与局部极小"></a>全局最小与局部极小</h2><p>我们常会谈到两种”最优”：”局部极小” (local minimum)和”全局最小” (global minimum). 对ω* 和θ*，若存在ε&gt;0 使得：</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/最优参数.png" style="zoom:100%;">

<p>都有E(ω ; θ) &gt;=  E(ω* ; θ* )成立，则(ω* ; 0* ) 为局部极小解;</p>
<p>若对参数空间中，任意(ω ; θ)，都有E(ω ; θ) &gt;=  E(ω* ; θ* )成立，则(ω* ; 0* ) 为全局最小解.</p>
<p>参数空间内梯度为零的点，只要其误差函数值小于邻点的误差函数值，就是局部极小点;可能存在多个局部极小值，但却只会有一个全局最小值.，我们在参数寻优过程中是希望找到全局最小.</p>
<p>基于梯度的搜索足使用最为广泛的参数寻优方法.在此类方法中，我们从某些初始解出发，迭代寻找最优参数值.每次迭代中，我们先计算误差函数在当前点的梯度，然后根据梯度确定搜索方向.由于负梯度方向是函数值下降最快的方向，因此梯度下降法就是沿着负梯度方向搜索最优解。若误差函数在当前点的梯度为零，则已达到局部极小，更新量将为零，这意味着参数的迭代更新将在此停止。若此极小值是陷入了局部极小， 这不是我们所希望的.</p>
<p>在现实任务中，人们常采用以下策略来试图”跳出”局部极小，从而进一步接近全局最小：<br>(1)以多组不同参数值初始化多个神经网络7 按标准方法训练后，取其中误差最小的解作为最终参数.这相当于从多个不同的初始点开始搜索， 这样就可能陷入不同的局部极小从中进行选择有可能获得更接近全局最小的结果.<br>(2)使用”模拟退火” (simulated annealing) 技术[Aarts and Korst, 1989].模拟退火在每一步都以二定的概率接受比当前解更差的结果，从而有助于”跳出”局部极小. 在每步迭代过程中，接受”次优解”的概率要随着时间的推移而逐渐降低从而保证算法稳定.(但是存在造成”跳出”全局最小的可能性)<br>(3)使用随机梯度下降.与标准梯度下降法精确计算梯度不同， 随机梯度下降法在计算梯度时加入了随机因素.于是即便陷入局部极小点，它计算出的梯度仍可能不为零，这样就有机会跳出局部极小继续搜索.</p>
<h2 id="其它常见神经网络概述"><a href="#其它常见神经网络概述" class="headerlink" title="其它常见神经网络概述"></a>其它常见神经网络概述</h2><h3 id="RBF网络"><a href="#RBF网络" class="headerlink" title="RBF网络"></a>RBF网络</h3><p>RBF(Radial Basis Function，径向基函数)网络[1988] 是一种单隐层前馈神经网络，它使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合.假定输入为d维向量风输出为实值，则RBF网络可表示为：</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RBF1.png" style="zoom:100%;">

<p>其中q为隐层神经元个数， Ci 和Wi 分别是第i个隐层神经元所对应的中心和权重， ρ(x,Ci) 是径向基函数，这是某种沿径向对称的标量函数，通常定义为样本x到数据中心Ci之间欧氏距离的单调函数。常用的高斯径向基函数形如:</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RBF2.png" style="zoom:100%;">

<p>通常采用两步过程来训练RBF网络:<br>第一步，确定神经元中心Ci ，常用的方式包括随机采样、聚类等;<br>第二步，利用BP算法等来确定参数ωi和βi.</p>
<h3 id="ART网络"><a href="#ART网络" class="headerlink" title="ART网络"></a>ART网络</h3><p>竞争型学习(competitive learning) 是神经网络中一种常用的无监督学习策略，在使用该策略时，网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制.这种机制亦称”胜者通吃” (winner-take-all) 原则.</p>
<p>ART(Adaptive Resonance Theory，自适应谐振理论)网络是竞争型学习的重要代表.该网络由比较层、识别层、识别阈值和重置模块构成.其中比较层负责接收输入样本，并将其传递给识别层神经元.识别层每个神经元对应1个模式类，神经元数目可在训练过程中动态增长以增加新的模式类.</p>
<p>在接收到比较层的输入信号后识别层神经元之间相互竞争以产生获胜神经元。经元.竞争的最简单力自式是计算输入向量与每个识别层神经元所对应的模式类的代表向量之间的距离，距离最小者胜.获胜神经元将向其他识别层神经元发送信号，抑制其撤活.若输入向量与获胜神经元所对应的代表向量之间的相似度大于识别阈值，则当前输入样本将被归为该代表向量所属类别，同时，网络连接权将会更新，使得以后在接收到相似输入样本时该模式类会计算出更大的相似度，从而使该获胜神经元有更大可能获胜;若相似度不大于识别阈值，则重置模块将在识别层增设一个新的神经元，其代表向量就设置为当前输入向量.</p>
<p>识别阙值对ART 网络的性能有重要影响.当识别阔值较高时，输入样本将会被分成比较多、比较精细的模式类，而如果识别阈值较低，则会产生比较少、比较粗略的模式类。</p>
<p>ART 比较好地缓解了竞争型学习中的”可塑性-稳定性窘境” (stability plasticity dilemma) ，可塑性是指神经网络要有学习新知识的能力，而稳定性则是指神经网络在学习新知识时要保持对旧知识的记忆.这就使得ART 网络具有一个很重要的优点:可进行增量学习(incremental learning)或在线学习(online learning).</p>
<h3 id="SOM网络"><a href="#SOM网络" class="headerlink" title="SOM网络"></a>SOM网络</h3><p>SOM(Self-Organizing Map ，自组织映射)网络是一种竞争学习型的无监督神经网络。它能将高维输入数据映射到低维空间(通常为二维) ，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元.</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/SOM.png" style="zoom:100%;">

<p>SOM网络中的输出层神经元以矩阵方式排列在二维空间中，每个神经元都拥有一个权向量，网络在接收输入向量后，将会确定输出层获胜神经元，它决定了该输入向量在低维空间中的位置. SOM 的训练目标就是为每个输出层神经元找到合适的权向量，以达到保持拓扑结构的目的.</p>
<p>SOM 的训练过程:在接收到一个训练样本后.每个输出层神经局会计算该样本与自身携带的权向量之间的距离，距离最近的神经元成为竞争获胜者，称为最佳匹配单元 (best matching unit). 然后，最佳匹配单元及其邻近神经元的权向量将被调整，以使得这些权向量与当前输入样本的距离缩小.这个过程不断迭代，直至收敛.</p>
<h3 id="级联相关神经网络"><a href="#级联相关神经网络" class="headerlink" title="级联相关神经网络"></a>级联相关神经网络</h3><p>级联相关网络有两个主要成分”级联”和”相关” 级联是指建立层次连接的层级结构.在开始训练时，网络只有输入层和输出层，处于最小拓扑结构;随着训练的进行，如下图所示，新的隐层神经元逐渐加入，从而创建起层级结构. 当新的隐层神经元加入时，其输入端连接权值是冻结固定的。相关是指通过最大化新神经元的输出与网络误差之间的相关性(correlation)来训练相关的参数.</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/级联相关神经网络.png" style="zoom:100%;">

<p>级联相关网络的训练过程：新的隐结点加入时，红色连接权通过最大化新结点的输出与网络误差之间的相关性来进行训练.</p>
<p>级联相关(Cascade-Correlation) 网络是结构自适应网络的重要代表，它的网络结构不是事先固定的，而是在训练的过程中学习的。</p>
<h3 id="Elman网络"><a href="#Elman网络" class="headerlink" title="Elman网络"></a>Elman网络</h3><p>与前馈神经网络不同，”递归神经网络” (recurrent neural networks) 允许网络中出现环形结构，从而可让一些神经元的输出反馈回来作为输入信号.这样的结构与信息反馈过程，使得网络在t 时刻的输出状态不仅与t时刻的输入<br>有关，还与t -1时刻的网络状态有关，从而能处理与时间有关的动态变化.</p>
<p>Elman 网络是最常用的递归神经网络之一，其结构如下图所示，它的结构与多层前馈网络很相似，但隐层神经元的输出被反馈回来，与下一时刻输入层神经元提供的信号一起，作为隐层神经元在下一时刻的输入.隐含层神经元通常采用Sigmoid激活函数，而网络的训练则常通过推广的BP 算法进行.</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Elman.png" style="zoom:100%;">

<h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><p>理论上来说，参数越多的模型复杂度越高、”容量” (capacity)越大，这意味着它能完成更复杂的学习任务.但一般情形下，复杂模型的训练效率低，易陷入过拟合。但随着云计算、大数据时代的到来，计算能力的大幅提高可缓解训练低效性，训练数据的大幅增加则可降低过拟合风险，因此，以”深度学习” (deep learning) 为代表的复杂模型开始受到人们的关注.</p>
<p>典型的深度学习模型就是很深层的神经网络.对神经网络模型，提高容量的一个简单办法是增加隐层的数目隐层多了，相应的神经元连接权、阈值等参数就会更多.<br>模型复杂度也可通过单纯增加隐层神经元的数目来实现，单隐层的多层前馈网络已具有很强大的学习能力;但从增加模型复杂度的角度来看，增加隐层的数目显然比增加隐层神经元的数目更有效，因为增加隐层数不仅增加了拥有激活函数的神经元数目，还增加了激活函数嵌套的层数.深度学习模型通常有八九层甚至更多隐层.<br>然而，多隐层神经网络难以直接用经典算法(例如标准BP 算法)进行训练，因为误差在多隐层内逆传播时，往往会”发散” (diverge)而不能收敛到稳定状态.</p>
<p>无监督逐层训练(unsupervised layer-wise training)是多隐层网络训练的有效手段，其基本思想是每次训练一层隐结点，训练时将上一层隐结点的输出作为输入，向本层隐结点的输出作为下一层隐结点的输入，这称为”预训练” (pre-training); 在顶训练全部完成后，再对整个网络进行”微调” (fine tuning)训练.比如，各层预训练完成后，再利用BP算法等对整个网络进行训练.</p>
<p>“预训练+微调”的做法可视为将大量参数分组，对每组先找到局部看来比较好的设置，然后再基于这些局部较优的结果联合起来远行全局寻优.这样就在利用了模型大量参数所提供的自由度的同时，有效地节省了训练开销.</p>
<p>另一种节省训练开销的策略是”权共享” (即让一组神经元使用相同的连接权.这个策略在卷积神经网(Convolutional Neural Network，简称CNN)中发挥了重要作用.</p>
<blockquote>
<p>注：卷积与池化：</p>
<p>卷积：</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/卷积1.png" style="zoom:100%;">

<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/卷积2.png" style="zoom:100%;">

<p>池化：</p>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/池化.png" style="zoom:80%;">
</blockquote>
<img src="/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN.png" style="zoom:80%;">

<p>如上图所示，网络输入是一个32 x 32 的手写数字图像，输出是其识别结果， CNN 复合多个” 卷积层”和”采样层”(池化)对输入信号进行加工，然后在连接层实现与输出目标之间的映射. 每个卷积层都包含多个特征映射(feature map) ， 每个特征映射是一个由多个神经元构成的“平面”，通过卷积滤波器提取输入的特征，一种卷积滤波器提取输入的一种特征。例如，上图中，第一个卷积层由6个特征映射构成，每个特征映射都是一个28*28的神经元阵列。其中每个神经元负责从5 x 5 的区域地通过卷积滤波器提取局部特征；(所以size由32-4=28)<br>采样层亦称为”汇合” (pooling)层， 其作用是基于局部相关性原理进行亚采样，从而在减少数据量的同时保留有用信息. 例如图中第一个采样层有6个14x14的特征映射，其中每个神经元与上一层中对应特征映射的2x2邻域相连，并据此计算输出(所以size由28/2=14)<br>通过复合卷积层和采样层，图中的CNN 将原始图像映射成120 维特征向量， 最后通过一个由84个神经元构成的连接层和输出层连接完成识别任务. CNN 可用BP算法进行训练，但在训练中无论是卷积层还是采样层， 其每一组神经元(即图中的每个” 平面” )都是用相同的连接权，从而大幅减少了需要训练的参数数目。</p>
<p>我们可以从另一个角度来理解深度学习. 其多隐层堆叠、每层对上一层的输出进行处理的机制，可看作是在对输入信号进行逐层加工，从而把初始的、与输出目标之间联系不太密切的输入表示，转化成与输出目标联系更密切的表示(或者说是更高维的信息)，使得原来仅基于最后一层输出映射难以完成的任务成为可能。换言之，通过多层处理，逐渐将初始的” 低层”特征表示转化为” 高层” 特征表示后， 用”简单模型” 即可完成复杂的分类等学习任务由此可将深度学习理解为进行”特征学习” (feature learning) 或” 表示学习” (representation learning) .</p>
<p>以往在机器学习用于现实任务时， 描述样本的特征通常需由人类专家来设计， 这称为” 特征工程”(feature engineering) . 特征的好坏对泛化性能有至关重要的影响，人类专家设计出好特征也并非易事;特征学习则通过机器学习技术自身来产生好特征，这使机器学习向”全自动数据分析”又前进了一步.</p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>神经网络领域的主流学术期刊有：<br>Neural Computation<br>Neural Networks<br>IEEE Transactions on Neural Networks and Learning Systems</p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      <a data-url="https://thexeme.github.io/2020/02/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" data-id="ck6vockxe0000astx69wkdr8x"
        class="article-share-link">分享</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>

    </footer>

  </div>

  
  
  <nav class="article-nav">
    
    
      <a href="/2020/02/17/%E5%86%B3%E7%AD%96%E6%A0%911/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">决策树1</div>
      </a>
    
  </nav>


  

  

  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2019-2020
        TheXeme
      </li>
      <li>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <ul class="list-inline">
  <li>PV:<span id="busuanzi_value_page_pv"></span></li>
  <li>UV:<span id="busuanzi_value_site_uv"></span></li>
</ul>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    
    <aside class="sidebar">
      
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/logo/xeme.png" alt="Xeme&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>谢谢，但是暂时无需打赏~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/pay/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/pay/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
			onClick: (e) => {
      	document.getElementById(e.target.innerText).scrollIntoView()
      	return false;
    	}
    });
  </script>


<script>
  var ayerConfig = {
    mathjax: 
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">



<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  
  

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

</html>